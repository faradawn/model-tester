# Notes on running the commands
- System: ARM CPU, Single NVIDIA GPU GB10, cuda capability 12.1. System has CUDA 13.
- use --quantization flag (modelopt_fp8 or modelopt_fp4) to fix the quantization issue
- use --mem-fraction-static 0.7 --disable-cuda-graph to avoid out of memory (ninja compiling kernel)
- If there are previous attempts, learn from them and construct a new docker command.
- Llama
    - RuntimeError: expand(CPUFloat8_e4m3fnType{[16, 512, 5120]}, size=[5120, 512]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)
    - Llama model not supported: tried triton and trtllm-mla backend, says ValueError: TRTLLM MHA backend is only supported on Blackwell GPUs (SM100). Please use a different backend.