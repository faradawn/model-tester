Model,Supported Status,Command,Note
nvidia/Qwen3-14B-FP8,Yes,"source .env && docker run --gpus all --shm-size 32g -p 30000:30000 -v ~/.cache/huggingface:/root/.cache/huggingface --env HF_TOKEN=""$HF_TOKEN"" --ipc=host lmsysorg/sglang:nightly-dev-cu13-20251208-599686b8 python3 -m sglang.launch_server --model-path nvidia/Qwen3-14B-FP8 --host 0.0.0.0 --port 30000 --quantization modelopt_fp8 --mem-fraction-static 0.7 --cuda-graph-max-bs 1",Need the --quantization flag (modelopt_fp8). Also need max cuda batch size and reduced mem-fraction-static. Otherwise might go out of memory during ninja build.
nvidia/Qwen3-32B-FP4,Yes,"source .env && docker run --gpus all --shm-size 32g -p 30000:30000 -v ~/.cache/huggingface:/root/.cache/huggingface --env HF_TOKEN=""$HF_TOKEN"" --ipc=host lmsysorg/sglang:nightly-dev-cu13-20251208-599686b8 python3 -m sglang.launch_server --model-path nvidia/Qwen3-32B-FP4 --host 0.0.0.0 --port 30000 --quantization modelopt_fp4 --mem-fraction-static 0.7 --cuda-graph-max-bs 1",Need the --quantization flag (modelopt_fp4). Also need max cuda batch size and reduced mem-fraction-static. Otherwise might go out of memory during ninja build.
nvidia/Phi-4-multimodal-instruct-FP8,Yes,"source .env && docker run --gpus all --shm-size 32g -p 30000:30000 -v ~/.cache/huggingface:/root/.cache/huggingface --env HF_TOKEN=""$HF_TOKEN"" --ipc=host lmsysorg/sglang:nightly-dev-cu13-20251208-599686b8 python3 -m sglang.launch_server --model-path nvidia/Phi-4-multimodal-instruct-FP8 --host 0.0.0.0 --port 30000 --quantization modelopt_fp8 --mem-fraction-static 0.7 --cuda-graph-max-bs 1 --trust-remote-code",Needs --trust-remote-code flag. Using max-bs 1 for faster testing
nvidia/Llama-4-Scout-17B-16E-Instruct-FP4,,"source .env && docker run --gpus all --shm-size 32g -p 30000:30000 -v ~/.cache/huggingface:/root/.cache/huggingface --env HF_TOKEN=""$HF_TOKEN"" --ipc=host lmsysorg/sglang:nightly-dev-cu13-20251208-599686b8 python3 -m sglang.launch_server --model-path nvidia/Llama-4-Scout-17B-16E-Instruct-FP4 --host 0.0.0.0 --port 30000 --quantization modelopt_fp4 --mem-fraction-static 0.7 --cuda-graph-max-bs 1",FP4 quantized Llama model with MoE architecture
nvidia/Qwen2.5-VL-7B-Instruct-FP8,,"source .env && docker run --gpus all --shm-size 32g -p 30000:30000 -v ~/.cache/huggingface:/root/.cache/huggingface --env HF_TOKEN=""$HF_TOKEN"" --ipc=host lmsysorg/sglang:nightly-dev-cu13-20251208-599686b8 python3 -m sglang.launch_server --model-path nvidia/Qwen2.5-VL-7B-Instruct-FP8 --host 0.0.0.0 --port 30000 --quantization modelopt_fp8 --mem-fraction-static 0.7 --cuda-graph-max-bs 1",FP8 quantized Qwen2.5 vision-language model

