Model,Quantization,Support Status,HF Handle
GPT-OSS-20B,MXFP4,,openai/gpt-oss-20b
GPT-OSS-120B,MXFP4,,openai/gpt-oss-120b
Llama-3.1-8B-Instruct,FP8,,nvidia/Llama-3.1-8B-Instruct-FP8
Llama-3.1-8B-Instruct,NVFP4,,nvidia/Llama-3.1-8B-Instruct-FP4
Llama-3.3-70B-Instruct,NVFP4,,nvidia/Llama-3.3-70B-Instruct-FP4
Qwen3-8B,FP8,,nvidia/Qwen3-8B-FP8
Qwen3-8B,NVFP4,,nvidia/Qwen3-8B-FP4
Qwen3-14B,FP8,Yes,nvidia/Qwen3-14B-FP8
Qwen3-14B,NVFP4,,nvidia/Qwen3-14B-FP4
Qwen3-32B,NVFP4,Yes,nvidia/Qwen3-32B-FP4
Phi-4-multimodal-instruct,FP8,Yes,nvidia/Phi-4-multimodal-instruct-FP8
Phi-4-multimodal-instruct,NVFP4,,nvidia/Phi-4-multimodal-instruct-FP4
Phi-4-reasoning-plus,FP8,,nvidia/Phi-4-reasoning-plus-FP8
Phi-4-reasoning-plus,NVFP4,,nvidia/Phi-4-reasoning-plus-FP4
Llama-3_3-Nemotron-Super-49B-v1_5,FP8,,nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-FP8
Qwen3-30B-A3B,NVFP4,,nvidia/Qwen3-30B-A3B-FP4
Qwen2.5-VL-7B-Instruct,FP8,,nvidia/Qwen2.5-VL-7B-Instruct-FP8
Qwen2.5-VL-7B-Instruct,NVFP4,,nvidia/Qwen2.5-VL-7B-Instruct-FP4
Llama-4-Scout-17B-16E-Instruct,NVFP4,No,nvidia/Llama-4-Scout-17B-16E-Instruct-FP4
Qwen3-235B-A22B (two Sparks only),NVFP4,,nvidia/Qwen3-235B-A22B-FP4
